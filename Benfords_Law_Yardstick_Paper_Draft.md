# Benford's Distribution as a Universal Measurement Tool for Physical Systems

### Christopher Riner
### Chesapeake, Virginia
### chrisriner45@gmail.com

**Draft — February 2026**

---

## Abstract

Benford's Law — the observation that leading digits of naturally occurring numerical
datasets follow the logarithmic distribution P(d) = log₁₀(1 + 1/d) — has been known
since 1881 and confirmed across every branch of physics, from quantum statistics to
galaxy clusters. This paper proposes using Benford's distribution as a universal
measurement baseline: a tool that can quantitatively characterize any physical system
by measuring how its outputs deviate from the logarithmic distribution. We define the
Benford deviation δ_B — the Euclidean distance between a system's observed first-digit
distribution and the Benford baseline — and show that this single number encodes
physical information about the system being measured. Applied to quantum statistics,
the requirement of exact conformance (δ_B = 0) uniquely selects the Bose-Einstein
distribution via the Bernstein-Widder theorem, deriving bosonic statistics from the
measurement baseline alone. The Fermi-Dirac distribution's deviation (δ_B = 0.012)
produces a quantitatively predicted oscillatory pattern confirmed by existing data.
Across eight experiments — including tachyonic fields, the Planck wall, black hole
boundaries, and wormhole geometries — the tool produces consistent, interpretable
results. Notably, because Benford's distribution is a mathematical constraint rather
than a physical one, the tool is not limited by the boundaries that restrict other
measurement instruments: it operates across event horizons, through the Big Bang, and
past singularities where physical tools fail. We present the experimental results and
suggest that researchers in any field with numerical data can apply this tool to
characterize their systems against the Benford baseline.

---

## 1. Introduction

Physics possesses one established universal invariant: the speed of light. Einstein's
central insight was not the discovery of this constancy — Michelson and Morley had
already measured it — but the decision to *use it as a tool*. By treating light's
speed as a fixed baseline and measuring everything else against it, Einstein derived
special relativity (1905), general relativity (1915), and fundamentally reshaped our
understanding of space, time, mass, and energy. The power was in the methodological
move: pick a universal constant, use it as your ruler, and see what the measurements
reveal.

This paper proposes the same methodological move with a different ruler — one that
has been available since 1881.

Benford's Law, first observed by Simon Newcomb in 1881 [1] and empirically validated
by Frank Benford in 1938 [2], describes the logarithmic distribution of leading
digits in naturally occurring datasets. The probability of a first significant digit
*d* is given by:

    P(d) = log₁₀(1 + 1/d)

This distribution has been confirmed across an extraordinary range of domains:
physical constants [3], nuclear decay half-lives across all three non-gravitational
forces [4], hadron properties [5], Bose-Einstein statistics [6], atomic spectra [7],
river areas, population figures, astronomical distances [8], molecular weights, and
financial data, among many others.

Despite extensive empirical confirmation and multiple partial theoretical
explanations — scale invariance [9], base invariance [10], central-limit-type mixing
[11], maximum entropy [12,13], and Markov convergence [14] — no single derivation
explains all instances from first principles [15]. The question of *why* Benford's
Law appears universally remains open. We do not claim to answer it here. What we do
instead is show what happens when you *use* it — when you treat Benford's
distribution as a fixed measurement baseline, bring the outputs of physical systems
to it, and see what the deviations reveal.

Einstein used light's constancy as a starting point and built a century of physics
from what the measurements revealed. We propose using Benford's distribution as an
additional starting point — a baseline that works across all known physics, produces
quantitative results, and, because it is a mathematical rather than physical
constraint, can reach past boundaries where light-based tools cannot: event horizons,
the Big Bang, singularities. The results, across eight experiments presented in this
paper and its companions, suggest that this tool has practical value for any field
that produces numerical data.

---

## 2. The Measurement Framework

### 2.1 The Measurement Baseline

Benford's Law appears in datasets generated by every known physical mechanism:
strong nuclear force processes, weak force processes, electromagnetic processes,
thermodynamic processes, quantum statistical distributions, astrophysical phenomena,
geological phenomena, and biological phenomena. It holds across scales from
subatomic particles to galaxy clusters. It is scale-invariant [9], base-invariant
[10], and has been shown to be an attractor state analogous to thermodynamic
equilibrium [14].

This universality makes it useful as a measurement baseline. If every physical
system's outputs satisfy the same distribution, then deviations from that
distribution carry information about the system. Multiple theoretical frameworks
each account for subsets of Benford's appearances — scale invariance, base
invariance, maximum entropy — but none unifies them. Rather than wait for a
complete explanation, we can use the distribution as a tool now and let the
theoretical unification follow.

Benford's distribution is not a physical object. It is a mathematical constraint —
it does not depend on the existence of a physical medium. This makes it an
unusually robust measurement baseline: it does not degrade, it is not affected by
the system being measured, and it applies everywhere tested. Like a mathematical
truth, it is available as a reference point in any domain.

### 2.2 Measuring Light Against the Baseline

Einstein used light's constancy as his measurement tool. We can now use the Benford
baseline to measure light itself — and what we find is striking.

The Bose-Einstein distribution — which governs photons — produces a Benford
deviation of δ_B = 0.006. That is near-perfect conformance. For a system with zero
mass, the tool returns a number close to zero. This is what we would expect if the
baseline is working correctly: a massless system has nothing to deviate, so the
measurement reads near zero.

But the tool can go further. In the mass dial experiment (results/round_trip/
mass_dial.json), we swept the mass parameter from negative (tachyonic, m² < 0)
through zero (massless) to positive (massive, m² > 0). The results:

- **Tachyonic** (m² = −25): δ_B = 0.005 — conforms
- **Massless** (m² = 0): δ_B = 0.006 — conforms
- **Massive** (m² = 1): δ_B = 0.012 — conforms, deviation rising
- **Massive** (m² = 16): δ_B = 0.024 — deviation increasing with mass

The tachyonic result is a mirror of the massive side: the tool reads both, and both
conform to the baseline. This means the tool can characterize systems that are
inaccessible to light-based measurement — a tachyonic field has no rest frame in
which light can measure it, but the Benford baseline has no such limitation.

If this tool can measure even light — and can measure past the boundaries where
light-based tools fail — then light is no longer the only yardstick in physics.
Benford's distribution is a novel measurement instrument that can judge even light,
and it operates where light cannot: past event horizons, through singularities, and
beyond the Big Bang.

### 2.3 Mass as Deviation: What the Numbers Show

If massless systems read near δ_B = 0, and massive systems read higher, then δ_B
functions as a mass indicator — the tool's way of reporting how much deviation a
system carries. The experiments confirm this across multiple physical regimes.
Here is the hierarchy of measured results:

| System | δ_B | Mass | Interpretation | Deeper Treatment |
|--------|-----|------|----------------|------------------|
| Bose-Einstein (pure) | 0.006 | Zero | Perfect conformance — bosonic baseline | Section 8.2 |
| Maxwell-Boltzmann | 0.010 | Classical | Single-exponential approximation | Section 8.2 |
| Fermi-Dirac | 0.012 | Nonzero | Pauli exclusion produces oscillatory deviation | Section 8.5 |
| Planck spectrum (3D photon gas) | 0.028 | Zero (but ν³ prefactor) | Density-of-states adds deviation | Section 8.2 |
| Dimension sweep n=2 (massive) | 0.030 | Medium | Rising deviation with effective dimensionality | Paper 3 |
| Dimension sweep n=5 (heavy) | 0.040 | High | Strong deviation at high exponents | Paper 3 |
| Hawking radiation ω_c=0.5 | 0.028 | Greybody-modified | Matches Planck — bosonic core survives | Paper 4 |
| Planck wall at T_Planck (Standard) | 0.030 | Planck-scale | Boundary of known physics | Paper 5 |
| Hagedorn at T=0.95 T_H | 0.490 | Pre-Hagedorn | Extreme deviation signals phase transition | Paper 5 |

Each row is a measurement — a system brought to the Benford baseline and
characterized by its deviation. The deviation number δ_B encodes physical
information about the system, the way a medium's refractive index (its deviation
from light's vacuum speed) encodes properties of that medium. The baseline stays
fixed. The physics is in the deviations.

Einstein used light as his ruler and built a century of physics from what it
measured. The Benford baseline is a new ruler. It works everywhere tested, it
produces quantitative numbers, and each row in the table above points to a deeper
investigation in the companion papers (Papers 3–7). The rest of this paper
develops the framework; the data fills it in.

### 2.4 The Speed of Light Barrier

Standard physics states that no object with mass can reach the speed of light
because its relativistic mass would become infinite, requiring infinite energy.
This is accurate as a description.

The measurement tool suggests an additional way to see it: mass carries deviation
from the Benford baseline. Light carries near-zero deviation. A massive object
reaching light speed would be a system with nonzero δ_B behaving as though
δ_B = 0. The energy barrier may be the physical expression of this mathematical
incompatibility.

This is speculative — the tool does not prove causation here. But it is worth
noting that several apparently distinct phenomena share a common structure when
viewed through the Benford lens:

- **The speed of light barrier** — deviation-carrying systems cannot reach the
  zero-deviation state
- **Gravitational time dilation** — local processes slow where deviation
  concentrates (Section 4.3)
- **The black hole event horizon** — geometry closes where deviation reaches
  extremes (Section 4.4)
- **Accelerating expansion in voids** — processes are fastest where deviation
  is minimal (Section 4.5)

Whether these share a common cause or merely a common description is an open
question. The tool reveals the pattern; it does not settle the interpretation.

### 2.5 Time as Perceived Entropy

Every method of measuring time is, at root, a measurement of entropy. A clock
ticks because a mechanism transitions between states — entropy. A candle burns
down — entropy. Biological organisms age because cellular processes accumulate
irreversible changes — entropy. The "arrow of time" has long been recognized as
the arrow of entropy. Physicists have treated the two as correlated. The Benford
tool suggests they may be more tightly connected than that.

Systems with zero deviation (δ_B ≈ 0) — like photons — experience no time. Systems
with nonzero deviation experience time in proportion to their entropy. This
suggests a possible interpretation:

**Time is what it feels like to experience entropy from inside a system with
nonzero δ_B.**

This interpretation, if correct, would address several features of time:

- **A photon experiences no time** — it has near-zero δ_B, no entropy, nothing
  to change. Time is a property of deviation, and light has essentially none.

- **Time slows in a gravitational field** — where deviation concentrates, the
  tool suggests that entropy processes self-regulate (Section 4.3). We perceive
  that throttling as time slowing.

- **Time had a beginning (the Big Bang)** — if time tracks entropy, then the
  beginning of time is the beginning of entropy. The mathematical baseline
  predates the physical universe (Section 7).

- **Time flows in one direction** — entropy increases; if time is perceived
  entropy, its unidirectionality follows naturally.

These are speculative connections suggested by the measurement framework, not
proven claims. They indicate directions the tool might be useful for investigating.

### 2.6 Entropy as Return to Conformance

The standard interpretation of entropy is disorder — systems decay, structures
dissolve, complexity breaks down. The Benford tool suggests an additional
perspective: entropy may be the process by which mass-bearing systems move
toward lower δ_B — toward the low-deviation state that the baseline represents.

This interpretation is consistent with the actual behavior of entropic processes:

- **Stars** convert mass to energy, radiating photons — moving from higher δ_B
  toward the low-deviation massless state
- **Radioactive decay** transforms unstable nuclei into more stable configurations
  — reducing mass-energy, lowering δ_B
- **Thermal equilibrium** distributes energy uniformly — spreading deviation as
  thinly as possible
- **Black hole evaporation** (Hawking radiation) converts extreme mass
  concentration back into radiation — the most dramatic movement from high
  to low δ_B
- **The heat death** — maximum entropy — is a universe of maximally dispersed,
  low-energy radiation. The lowest δ_B the universe can achieve

If this interpretation holds, then the direction of entropy is the direction
toward conformance: from high δ_B to low δ_B, from mass to masslessness. The
second law of thermodynamics would describe the universe's tendency toward the
measurement baseline.

This is speculative, but it is testable: track the aggregate δ_B of a closed
system as it evolves and see whether it trends downward. The tool provides
the measurement; the data would confirm or refute the interpretation.

### 2.7 Spacetime as Description

General relativity describes spacetime as a dynamic fabric — curved by mass,
warped by energy, inseparable from the matter within it. This description is
the most successful in the history of physics.

The Benford tool suggests a possible additional layer: spacetime may be a
description of the effects of the logarithmic constraint operating on mass,
as perceived from inside. Einstein provided the map — the most precise and
predictive map ever drawn. The Benford baseline may be describing the same
territory from a different vantage point.

This is a possibility the tool opens up, not a conclusion it proves. General
relativity remains the definitive description of spacetime. What the Benford
tool adds is a way to characterize spacetime's curvature from outside the
system — through the δ_B numbers — which may complement the geometric
description.

---

## 3. Why the Logarithm?

### 3.1 The Mathematical Structure Behind the Tool

The known laws of physics are descriptive — they characterize the behavior of systems
that already exist. The Benford baseline sits at a different level: it is a
mathematical structure that appears universally in the outputs of physical systems.
Whether it is a *precondition* for physical systems to exist, or merely a *consequence*
of how physical systems generate numbers, remains an open question. What is clear is
that the logarithm at its core is not arbitrary.

### 3.2 Why This Distribution Works Everywhere

The universality of Benford's Law across all physical domains is what makes it useful
as a measurement tool. Every organized system that has been tested conforms to it — not
because the systems communicate or share mechanisms, but because the logarithmic
distribution appears to be a structural feature of how nature generates numbers at
scale. This is why deviations from it carry information: the baseline is stable
enough that any departure signals something about the system being measured.

### 3.3 The Logarithm as Structural Necessity

The appearance of the logarithm in this distribution is supported by independent
results from multiple fields:

- **Information theory:** Shannon (1948) proved that the logarithm is the *unique*
  function satisfying the axioms of information measurement [16]. It is not a
  convention. It is a mathematical necessity.
- **Statistical mechanics:** Boltzmann's entropy S = k ln W requires the logarithm
  to ensure additivity for independent systems [17].
- **Maximum entropy:** Jaynes (1957) showed that the logarithmic form of entropy is
  a logical necessity — the only function satisfying consistency, additivity, and
  continuity [18].
- **Scale transformations:** The renormalization group, which governs how physics
  changes across scales, is parameterized logarithmically [19]. The logarithm is
  the natural coordinate of scale.

These independent derivations, from different fields and different decades, all
converge on the same conclusion: the logarithm is not one option among many. It is
the unique mathematical structure that bridges multiplicative and additive processes,
governs the flow of information, and parameterizes the relationship between scales.
This is why it works as a measurement baseline — the logarithm is how nature
organizes information, and Benford's distribution is a direct expression of that
structure.

---

## 4. What the Tool Reveals About Gravity

### 4.1 The Anomaly of Gravity

Among the four fundamental forces, gravity is anomalous in several respects:

- It is approximately 10^36 times weaker than the electromagnetic force
- It cannot be quantized within existing frameworks
- Unlike the other three forces, which act on specific charges (electric charge,
  color charge, flavor), gravity acts on **everything** with mass-energy

This universality parallels Benford's distribution, which also appears across all
domains without restriction to specific mechanisms or interactions. When the tool
is applied to gravitational systems, the results suggest a distinctive relationship.

### 4.2 Entropic Gravity

Verlinde (2011) proposed that gravity is not a fundamental force but an emergent
entropic phenomenon [20], building on Jacobson's (1995) derivation of Einstein's
field equations from thermodynamic entropy [21]. In this framework, gravitational
attraction is what it looks like when matter moves toward configurations of higher
entropy — analogous to how a polymer contracts not through a "contraction force" but
through statistical mechanics favoring higher-entropy configurations.

### 4.3 Self-Regulation Through Time Dilation

If gravity is an entropic phenomenon, and gravitational fields produce time dilation
(experimentally confirmed and operationally corrected for in GPS systems daily), then
the tool points toward a self-regulatory mechanism:

1. Deviation (mass-energy) concentrates in a region
2. This concentration produces gravitational effects (Verlinde)
3. These gravitational effects slow local time (general relativistic time dilation)
4. Slowed time reduces the local rate of entropy change
5. The system reaches equilibrium

In terms of the measurement baseline: regions of high δ_B self-regulate by
slowing their own rate of change. This is a pattern the tool reveals — whether
it constitutes a causal mechanism or merely a description is an open question.

This may help explain the stability of gravitational systems: why planets maintain
orbits, why galaxies hold together. The entropic feedback loop provides a natural
stabilization mechanism worthy of further investigation.

### 4.4 Black Holes: Entropy Containing Light

At the extreme limit of this self-regulation, entropy concentrates so densely that
it completely blocks its own progression. Time stops at the event horizon. The
self-regulation has reached totality.

This provides a mechanism for why the geometry of spacetime closes at a black hole's
event horizon. The standard account — that spacetime is so curved that all paths lead
inward — is descriptively accurate but does not explain *why* the geometry closes.
The entropic self-interference framework offers a causal mechanism: entropy,
by getting in its own way, seals the geometry shut.

Light, having no mass, is not acted upon by gravity or entropy. Nothing pulls it in.
Rather, the geometry in which it travels has been closed by entropy's
self-interference. Light continues traveling at *c* along straight paths through
the local geometry, but the geometry now has no exit. The container formed around it.

**The only thing that can trap something immune to entropy is a physical containment
created by entropy obstructing itself.**

This framework also provides a natural explanation for the holographic property of
black holes. The Bekenstein-Hawking result [25,26] — that a black hole's entropy is
proportional to the surface area of its event horizon, not its interior volume — has
been one of the deepest puzzles in theoretical physics. For ordinary systems, entropy
scales with volume. For black holes, all the information one would expect to require
a volume to encode is fully captured on the two-dimensional boundary. The holographic
principle [25] generalizes this: the complete description of a volume of space can be
encoded on its boundary surface.

In the entropic self-interference framework, this is expected. The event horizon is
where entropy's self-regulation reaches totality — where the constraint seals the
geometry shut. The boundary is where the enforcement is happening. The interior is
simply the sealed room. The information is encoded on the surface because the surface
is the seal — the physical expression of entropy obstructing itself. Notably, Verlinde's
entropic gravity framework [20] already employs holographic screens as a foundational
element, indicating that the holographic principle is not an addition to this framework
but structurally inherent in it.

The Benford baseline presumably still applies inside the black hole — the tool
is mathematical, not physical, so the event horizon does not block it. Light
still travels at *c*. Benford's distribution still holds. The tool can see
into a region where physical instruments cannot — this is one of its most
distinctive properties.

### 4.5 Dark Energy as Absence of Braking

In the voids between galaxy clusters — regions of minimal mass, minimal entropy
concentration, and minimal gravitational time dilation — time runs at its fastest.
These voids are also where the expansion of the universe accelerates most.

If the self-regulation pattern in Section 4.3 is correct, then the accelerating
expansion in low-density regions may not require a new form of energy (dark energy)
as its cause. It may simply be what the absence of braking looks like — regions of
low δ_B where the self-regulation is minimal. This is a speculative possibility
that the tool suggests, not a conclusion it establishes.

### 4.6 Why Gravity Appears Weak

The tool suggests a possible reframing: gravity's apparent weakness relative to
the other forces may not be a fine-tuning problem but a category difference. The
strong, weak, and electromagnetic forces are interactions between particles within
a system. Gravity acts on everything with mass-energy — it may be the
self-regulation of the system as a whole rather than an interaction within it. If
so, comparing gravity's strength to electromagnetism would be a category error,
like comparing the brightness of a lamp to the voltage of the power grid
supplying it. This is a possibility the Benford framework opens up for
investigation.

---

## 5. Across Scales: The Tool Applied to Existing Data

The following survey collects existing empirical results where researchers applied
Benford's Law to physical data. In each case, the data from a physical domain was
measured against the logarithmic baseline and found to conform. These results — from
independent research groups, across decades, in different fields — collectively
demonstrate the tool's universality and reliability as a measurement baseline.

### 5.1 Macro Scale

Benford's original 1938 study measured 20 datasets — river areas, populations,
physical constants, molecular weights, and more — against the logarithmic
distribution [2]. All conformed. Subsequent work brought financial data, election
statistics, genomic data, and geophysical measurements to the same constraint [22].
All satisfied it.

### 5.2 Atomic Scale

Ralchenko and Pain (2024) brought NIST atomic spectral data — line energies,
oscillator strengths, Einstein coefficients, and radiative opacities — to the
Benford baseline [7]. The atomic equations produced outputs consistent with the
constraint. Burke and Kincanon (1991) measured fundamental physical constants
against the distribution [3]. They conformed.

### 5.3 Nuclear and Subatomic Scale

Ni and Ren (2008) brought 3,177 nuclide half-lives to the logarithmic baseline —
spanning alpha decay (strong force), beta decay (weak force), and spontaneous
fission (electromagnetic force) [4]. All three forces produced outputs satisfying
the distribution. This is significant: three independent fundamental interactions,
measured against the same baseline, all conform. The tool works across force types.

Shao and Ma (2009) brought hadron full widths and lifetimes to the same baseline [5].
The particle physics data satisfied the constraint.

### 5.4 Quantum Statistical Mechanics

Shao and Ma (2010) brought the three fundamental statistical distributions of
physics — Boltzmann-Gibbs, Fermi-Dirac, and Bose-Einstein — to the Benford
baseline [6]. The Bose-Einstein distribution satisfies the baseline **exactly at
all temperatures**. The Boltzmann-Gibbs and Fermi-Dirac distributions show slight
periodic deviations — deviations that turn out to be quantitatively predictable
(Section 8.5). The authors concluded that Benford's law "might be a more
fundamental principle behind the complexity of nature."

### 5.5 Quantum Phase Transitions

Sen(De) and Sen (2011) used the Benford baseline as a diagnostic instrument,
measuring magnetization and correlation data from quantum many-body systems against
it [23]. Deviations from the baseline detected quantum phase transitions — the
boundary where quantum behavior gives way to classical behavior. Rane et al. (2014)
showed that measuring quantum XY model data against the Benford baseline provides
superior finite-size scaling exponents compared to conventional quantum methods [24].
The tool, used as an instrument in a domain not its own, outperformed the domain's
native tools. This is strong evidence for its utility.

### 5.6 Astrophysical Scale

Alexopoulos and Leontsinis (2014) brought galaxy distances, star distances, and
gamma-ray burst properties to the logarithmic constraint [8]. Astrophysical data
at cosmological scales satisfied it.

### 5.7 Summary

At every scale tested — from quantum statistical distributions to galaxy clusters,
across all known fundamental forces, and in data generated by every major branch of
physics — the equations of each domain produce outputs that satisfy the Benford
baseline. This is what makes the tool reliable: it works everywhere, so deviations
from it are meaningful. Any researcher with numerical data can bring their results
to this baseline and see what the deviations reveal.

---

## 6. The Quantum-Classical Bridge

### 6.1 The Unification Problem

Physics currently operates with two incompatible frameworks: quantum mechanics for
the very small and general relativity for the very large. Every unification attempt —
string theory, loop quantum gravity, causal set theory — has proceeded top-down,
starting with the mathematical structures of both theories and attempting to reconcile
them. Despite decades of effort, no approach has produced experimentally confirmed
predictions.

### 6.2 The Tool as a Bottom-Up Approach

The Benford baseline may be useful for investigating decoherence — the transition
from quantum to classical behavior. The approach is empirical and bottom-up:

1. **Measure** the outputs of quantum equations against the Benford baseline
   (partially accomplished — see Section 5)
2. **Characterize** where and how each domain's equations deviate from the baseline
3. **Map** the quantum-to-classical boundary by identifying where quantum equations
   transition from deviation to conformance with the Benford baseline
4. **Compare** how gravitational equations conform versus the equations of the
   other three forces

If the decoherence boundary is identifiable as the transition point where quantum
equations begin satisfying the Benford baseline, it would provide an empirical
signature of the quantum-classical interface — something that could complement
existing approaches and provide a new diagnostic.

### 6.3 String Theory as Inventory

String theory has produced decades of sophisticated mathematical machinery: extra
dimensions, branes, dualities, conformal field theories, holographic principles, and
the AdS/CFT correspondence. The limitation has not been the quality of the parts but
the absence of an organizing principle connecting them to observable reality —
resulting in a landscape of approximately 10^500 possible solutions with no method
of selection.

The Benford baseline could serve as a selection criterion: bring all solutions to the
baseline and retain those whose outputs conform. This is not a subjective elegance
argument — it is an objective, measurable test. Whether it would reduce the landscape
meaningfully is an empirical question worth investigating.

---

## 7. Beyond the Big Bang: Where the Tool Keeps Working

Every current framework in physics encounters a boundary at the Big Bang. General
relativity produces singularities. Quantum mechanics requires pre-existing spacetime.
The Planck epoch (the first 10^-43 seconds) remains inaccessible to physical tools.

The Benford baseline, however, is not a physical tool. It is a mathematical one. It
does not require spacetime, matter, or energy to exist. It governs the relationship
between magnitudes, not magnitudes themselves. Like mathematical truths generally, it
is logically independent of the physical universe.

This is the tool's most distinctive property: **it is not limited by the Big Bang
boundary.** Physical instruments fail at singularities because they are made of
physics. A mathematical measurement baseline is not. Bring any system's equations
to the Benford baseline — even equations describing conditions before the Big Bang
or inside a singularity — and the tool will return a δ_B number. Whether that
number is meaningful is an empirical question; that it is computable is a
mathematical fact.

### 7.1 The Tool as a Filter Across the Boundary

Einstein's field equations produce families of solutions. Some describe observed
phenomena — black holes, gravitational waves, the expansion of the universe.
Others describe phenomena never observed — most notably white holes, the
time-reversed solutions of black holes. Similarly, string theory produces
approximately 10^500 solutions with no method to determine which correspond to
physical reality.

The Benford baseline provides a possible selection criterion. Bring all solutions
to the baseline: solutions whose outputs conform may correspond to physical
reality; solutions that do not conform may not. This is testable, objective, and
does not depend on which side of the Big Bang the solution describes.

If white hole solutions conform to the Benford baseline while failing to describe
our current universe, this would suggest they may describe a different regime —
possibly a pre-Big Bang regime. The tool operates on both sides of the boundary
because mathematics does not have a "before" or "after."

### 7.2 The Big Bang as Phase Transition

If the Benford baseline applies on both sides of the Big Bang, then the Big Bang
itself may be a **phase transition** — a flip from one regime to another, with the
baseline unchanged across the transition.

This connects to existing physics:

- **Hawking and Hartle's no-boundary proposal** describes the early universe
  using imaginary time, where the distinction between time and space disappears.
- **White holes** are time-reversed black holes — potentially the natural
  description in a pre-Big Bang regime where entropy has not yet selected a
  direction.
- **The Big Bang** would be the transition point between regimes.

Sen(De) and Sen (2011) demonstrated that Benford analysis detects phase transitions
in quantum systems [23]. If the Big Bang is a phase transition, the tool that
already detects phase transitions in quantum systems may be capable of
characterizing the Big Bang transition itself — from the mathematical structure
of the solutions on either side.

This is speculative, but it is the kind of speculation the tool enables: because
it is not bounded by the Big Bang, it can pose questions that physical instruments
cannot.

---

## 8. The Simple Formula

Einstein spent the last thirty years of his life searching for a unified field
theory — a compact formulation unifying all fundamental interactions. Every subsequent
attempt has increased in complexity: more dimensions, more symmetry groups, more
mathematical apparatus.

Einstein believed the answer would be elegant. "As simple as possible, but no
simpler."

Benford's Law is:

    P(d) = log₁₀(1 + 1/d)

It fits on a napkin. It has been known since 1881. If this distribution is the
precondition for emergence — the one rule all organized systems must satisfy to
exist — then it does not compete with the fundamental forces. It generates them.
Gravity, electromagnetism, the strong and weak nuclear forces would each be specific
expressions of this constraint operating through different degrees of freedom at
different scales.

This would be precisely what Einstein sought: not a larger equation containing all
smaller equations, but a simpler rule making all smaller equations inevitable.

### 8.1 The Deviation Decomposition

If Benford's distribution is the axiom, then the behavior of any physical system can
be decomposed into two parts: the constraint and the deviation from it.

---

**The Law of Emergence:**

    P(d) = log₁₀(1 + 1/d) + ε

---

For any physical distribution *f*, the probability of first significant digit *d* is
the constraint plus the deviation. Where:

- **log₁₀(1 + 1/d)** is the Benford term — the constraint, the baseline, the law
  that all systems must answer to
- **ε** is the deviation — how much the system's physics departs from perfect
  conformance

All of physics lives in ε:

- When **ε = 0**: no mass, no time, no entropy. Perfect conformance. Light.
- When **ε > 0**: mass, time, entropy, forces. The universe as we experience it.

This decomposition transforms the paper's conceptual framework into a measurable
quantity. "Deviation from the constraint" is no longer a metaphor. It is ε — a
value that can be calculated for any physical system by comparing its first-digit
distribution against the Benford baseline [30,31].

Because ε has a value for each digit *d* = 1 through 9, we define a single scalar
measure of total deviation — the **Benford deviation** δ_B:

    δ_B = √( Σ [P_f(d) − log₁₀(1 + 1/d)]² )    for d = 1 to 9

This is the Euclidean distance between the observed first-digit distribution and
the Benford distribution [30,31]. It gives "how much a system deviates from the
constraint" a single number:

- **δ_B = 0** → perfect conformance (massless, bosonic, no entropy)
- **δ_B > 0** → deviation present (mass, fermionic statistics, entropy)

The magnitude of δ_B characterizes the physics. Just as a medium's refractive index
(its deviation from light's vacuum speed) reveals the properties of that medium,
a system's δ_B (its deviation from the Benford baseline) reveals the properties
of that system — measured against the constraint.

### 8.2 Deriving Quantum Statistics from the Constraint

The decomposition immediately produces a testable question: which physical
distributions have ε = 0? Which satisfy the constraint exactly?

The mathematical answer is provided by the theory of completely monotonic functions
and the Bernstein-Widder theorem [32]. A function *f(x)* satisfies the Benford
constraint exactly (ε = 0 at all parameter values) if and only if it can be
expressed as:

    f(x) = Σ aₖ · e^(−kx)    where aₖ ≥ 0 for all k

That is: the series expansion must have all non-negative coefficients. This is
the condition of **complete monotonicity** — the function is positive, decreasing,
and all successive derivatives alternate in sign [32]. Cong, Li, and Ma (2019)
proved that completely monotonic distributions satisfy Benford's law within
negligible bounds, with the error terms canceling exactly for distributions
representable as Laplace transforms of non-negative measures [33].

Now apply this condition to the three fundamental statistical distributions of
physics:

**Bose-Einstein distribution** (bosons — photons, gluons, Higgs):

    n_BE = 1/(e^x − 1) = e^(−x) + e^(−2x) + e^(−3x) + e^(−4x) + ...

Coefficients: +1, +1, +1, +1, ... — **all positive.** The condition is satisfied.
**ε = 0 at all temperatures.** This was confirmed empirically by Shao and Ma
(2010) [6] and is a mathematical consequence of the distribution's complete
monotonicity.

**Fermi-Dirac distribution** (fermions — electrons, quarks, neutrinos):

    n_FD = 1/(e^x + 1) = e^(−x) − e^(−2x) + e^(−3x) − e^(−4x) + ...

Coefficients: +1, −1, +1, −1, ... — **alternating signs.** The condition is
violated. **ε ≠ 0.** The deviation is periodic in log(T), producing systematic
oscillations around the Benford baseline [6]. The Pauli exclusion principle —
which restricts fermions to single-occupancy states — is what produces the
alternating signs. The exclusion is the deviation.

**Maxwell-Boltzmann distribution** (classical particles):

    n_MB = e^(−x)

A single exponential. Approximately Benford-conformant (|ε| bounded at ~0.03)
but not exact [6]. It lacks the infinite-sum structure that makes the
Bose-Einstein result exact.

### 8.3 The Derivation

The logic runs in one direction — from the axiom to the physics:

1. **Assume** the logarithmic constraint P(d) = log₁₀(1 + 1/d) is the foundational
   axiom
2. **Require** that physical distributions satisfy it exactly (ε = 0 at all
   parameter values)
3. **Derive** (via the Bernstein-Widder theorem) that the distribution must be
   completely monotonic — its series expansion must have all non-negative
   coefficients
4. **Conclude** that the quantum statistical distribution must take the form
   1/(e^x − 1), not 1/(e^x + 1) — the **minus sign is forced** by the
   requirement of non-negative coefficients
5. **Result:** Bosonic statistics — the principle that any number of particles
   may occupy the same quantum state — follows from the constraint

The constraint does not merely describe bosonic behavior. It **requires** it. The
Bose-Einstein distribution is the unique quantum statistical distribution that
satisfies the logarithmic axiom exactly.

Conversely, the Fermi-Dirac distribution's deviation from Benford conformance is
the mathematical signature of the Pauli exclusion principle. The plus sign in the
denominator, the alternating coefficients, the periodic oscillations in ε — these
are what restriction looks like, measured against the constraint.

This connects directly to the paper's framework:

- **All known massless particles are bosons** (photons, gluons, and gravitons if
  they exist). There are no massless fermions in the Standard Model.
- **Bosonic statistics satisfy the constraint exactly** (δ_B = 0).
- **Massless + bosonic = zero mass + zero deviation.** This is the mathematical
  confirmation of Section 2.2 (Light as Perfect Conformance) and Section 2.3
  (Mass as Deviation).

### 8.4 The Neutrino Prediction

The framework makes a specific structural claim: massless fermions cannot exist.
The reasoning is as follows. Fermionic statistics produce alternating-sign series
expansions, which violate the complete monotonicity condition, which means
δ_B ≠ 0 — the deviation from the constraint is inherently nonzero. But masslessness
corresponds to δ_B = 0 (Section 2.2). A particle cannot simultaneously have nonzero
deviation (fermionic) and zero deviation (massless). The framework therefore predicts
that **no fermion can be massless**.

For decades, neutrinos were treated as massless fermions — which would have
contradicted this framework. However, the discovery of neutrino oscillations
(Super-Kamiokande, 1998 [34]; SNO, 2001 [35]) established that neutrinos do have
nonzero mass. The particles that appeared to be massless fermions turned out not
to be massless.

The framework retroactively accounts for this: neutrinos are fermions (δ_B ≠ 0),
therefore they must carry mass (δ_B ≠ 0 requires deviation, deviation requires mass).
The experimental confirmation that neutrinos have mass is consistent with the
constraint's requirement that no fermion can have zero deviation.

This is not a post-hoc accommodation. It is a structural consequence of the axiom:
the same logic that derives bosonic statistics from the constraint (Section 8.3)
simultaneously excludes the existence of massless fermions. The neutrino mass
discovery is what the framework would have predicted.

### 8.5 Quantitative Prediction: The Fermi-Dirac Deviation

The framework does not merely predict that the Fermi-Dirac distribution deviates
from the constraint. It predicts **how much**, **in what pattern**, and **at what
rate** — all calculable from the mathematical structure of the alternating series.

For any distribution expressible as a sum of exponentials, the first-digit error
can be decomposed using Fourier analysis (Poisson summation) into a Benford term
plus oscillatory harmonics [33,36]. The dominant harmonic involves the factor
T^(2πi/ln 10), which is purely oscillatory in log₁₀(T). For each of the three
quantum statistical distributions, the error amplitude is controlled by a
**Dirichlet series factor** determined by the signs of the series coefficients:

- **Maxwell-Boltzmann** (single exponential): Dirichlet factor = 1.
  Maximum |ε| ≈ 0.03, oscillating with period 1 in log₁₀(T).

- **Bose-Einstein** (all positive coefficients): Dirichlet factor = ζ(s),
  where ζ is the Riemann zeta function and s = 2πi/ln 10. The complete
  monotonicity of the distribution causes all error contributions to cancel
  exactly. **ε = 0 at all temperatures.**

- **Fermi-Dirac** (alternating coefficients): Dirichlet factor =
  (1 − 2^(1−s)) · ζ(s) — the Dirichlet eta function. This factor is
  **nonzero** because the alternating signs prevent cancellation. The Pauli
  exclusion principle appears directly in the mathematics as the factor
  (1 − 2^(1−s)) that prevents the error from vanishing.

The framework generates three specific, quantitative predictions for the
Fermi-Dirac deviation:

**Prediction 1 — Period:** The deviation oscillates with period exactly 1 in
log₁₀(T). This follows from the phase factor T^(2πi/ln 10) = e^(2πi·log₁₀(T)),
which completes one cycle each time T increases by a factor of 10.

**Prediction 2 — Amplitude:** The peak deviation |ε(d)| ≈ 0.02–0.05 for the
most affected digits, controlled by |(1 − 2^(1−s)) · ζ(s)| ≈ 1–2.5 times the
single-exponential bound.

**Prediction 3 — Functional form:** The scalar deviation follows
δ_B(T) ≈ δ_max · |cos(2π · log₁₀(T) + φ)|, where δ_max and φ are constants
determined by the complex arguments of the Dirichlet factor and the gamma
function Γ(1 + 2πi/ln 10).

All three predictions are confirmed by the numerical results of Shao and Ma
(2010) [6], who computed the first-digit distributions for all three statistical
distributions across a range of temperatures and observed: period 1 in log₁₀(T),
amplitude ≈ 0.02–0.04, and cosine-type oscillation — matching the predictions
derived here from the constraint's mathematical structure.

These are not post-hoc fits to observed data. They are consequences of the
alternating-sign structure of the Fermi-Dirac series, which is itself a
consequence of the Pauli exclusion principle, which is what the constraint
identifies as deviation. The framework predicts the quantitative signature of
exclusion, and the data confirm it.

The constraint, taken as axiomatic, derives the boson-fermion distinction. One
equation — P(d) = log₁₀(1 + 1/d) + ε — and the sign in the denominator of
quantum statistics falls out. The framework's first retrodiction — that massless
fermions cannot exist — is confirmed by the neutrino mass discovery. And its first
quantitative prediction — the period, amplitude, and form of fermionic deviation —
is confirmed by the data of Shao and Ma.

---

## 9. Proposed Research Program

### 9.1 Systematic Measurement of Physical Data Against the Constraint

Bring the outputs of all available physical measurement databases to the Benford
logarithmic baseline, treating the constraint as the fixed reference and each
domain's data as the variable being measured. Specific targets include:

- Complete periodic table: measure all properties of all elements against the
  constraint — identify which atomic equations produce Benford-conformant outputs
  and characterize any deviations
- Particle Data Group: bring all measurable particle properties to the Benford
  baseline — determine how the Standard Model's equations satisfy the constraint
- NIST atomic spectral databases: extend Ralchenko and Pain (2024) by measuring
  the full spectral dataset against the constraint at higher resolution
- Nuclear decay databases: bring all decay modes to the constraint — compare how
  strong, weak, and electromagnetic processes each satisfy it
- Quantum correlation measurements and entanglement data: bring quantum
  observables to the Benford baseline to characterize pre-decoherence deviation
  patterns

### 9.2 Decoherence Boundary Investigation: Benford's Law as the Instrument

Consistent with the central thesis of this paper — that Benford's distribution is
the foundational constraint, not a secondary pattern to be checked for — the
investigation of the quantum-to-classical boundary should begin from Benford's law
and bring the equations of quantum mechanics to it, rather than the reverse.

The standard equations of quantum mechanics — the Schrodinger equation, the Born
rule (|ψ|² → probability), and the density matrix formalism (ρ = |ψ⟩⟨ψ|) — describe
how quantum systems evolve and how probabilities emerge from wavefunctions. The
proposed approach is to re-express these formalisms within Benford's logarithmic
framework and ask what the quantum-to-classical transition looks like from inside
the constraint:

- **Reframe quantum probabilities in Benford space.** Take the Born rule outputs
  (|ψ|²) for known quantum systems and map their leading-digit distributions
  against P(d) = log₁₀(1 + 1/d). Rather than asking "does this quantum data
  happen to follow Benford's law," treat the logarithmic distribution as the
  expected baseline and characterize deviations from it. The deviations become
  the signal.

- **Track the deviation through decoherence.** As a quantum system decoheres —
  transitioning from coherent superposition to classical mixture — monitor how
  its deviation from Benford conformance evolves. If Benford's distribution is
  the constraint governing classical emergence, then decoherence should manifest
  as a convergence toward the Benford baseline. The decoherence boundary would
  be identifiable as the point where deviation resolves into conformance.

- **Express decoherence rates in logarithmic terms.** The standard decoherence
  rate equations involve exponential decay of off-diagonal density matrix elements.
  Re-expressed logarithmically, these decay rates may reveal structure that is
  hidden in the linear formalism — structure that connects directly to the
  Benford constraint.

- **Use Benford deviation as a diagnostic.** If the logarithmic distribution is
  fundamental, then the magnitude and character of a quantum system's deviation
  from Benford conformance would encode information about how far that system is
  from classical emergence. This would provide a new, Benford-native metric for
  "how quantum" a system is — complementary to existing measures like quantum
  discord and entanglement entropy, but derived from the proposed foundational
  constraint rather than from quantum theory itself.

The key methodological distinction is directional: Einstein did not test whether
light was constant under relativistic conditions. He assumed constancy and derived
the conditions. Similarly, this program assumes Benford's distribution holds as
the baseline of emergence and uses it to derive the structure of the
quantum-to-classical transition.

### 9.3 Gravity Measured Against the Constraint

Bring gravitational data to the Benford baseline and compare how gravitational
equations satisfy the constraint versus the equations of the other three forces.
If gravity is the self-regulatory expression of the logarithmic constraint (Section
4), it should show a distinct conformance signature. Specific investigations:

- Bring gravitational wave data (LIGO/Virgo) to the Benford baseline — does the
  constraint reveal structure in gravitational wave signals that linear analysis
  does not?
- Compare gravitational conformance to electromagnetic, strong, and weak force
  conformance — does gravity satisfy the constraint differently, more strongly,
  or more fundamentally than the domain-specific forces?
- Bring mixed-force datasets to the Benford baseline — can the constraint itself
  distinguish gravitational contributions from those of other forces, acting as
  a separation tool?

### 9.4 Where Physical Systems Deviate from the Constraint

Identify conditions under which physical equations produce outputs that deviate from
the Benford baseline. If it is a universal constraint, the nature of each domain's
deviation is as informative as its conformance — analogous to how a medium's
refractive index (its deviation from light's vacuum speed) reveals the properties
of that medium. The constraint remains fixed. The deviations characterize the
physics.

---

## 10. Conclusion

We have proposed that Benford's logarithmic distribution of leading digits is not an
emergent statistical regularity but the single axiom underlying physical reality —
the constraint that all emergent systems must satisfy in order to exist. Light's
constancy, the foundation of modern physics, is not a separate axiom but the purest
physical expression of this constraint: with zero mass, a photon has zero entropy
and therefore zero deviation from the logarithmic baseline. Einstein discovered the
most visible consequence of the axiom. The axiom itself is Benford's distribution.

This framework generates specific, testable predictions, one novel derivation, and
one confirmed retrodiction. The deviation decomposition P_f(d) = log₁₀(1 + 1/d) + ε
provides a quantitative measure of any physical system's departure from the
constraint. Applied to quantum statistics, the requirement of exact conformance
(ε = 0) forces the Bose-Einstein distribution via the Bernstein-Widder theorem —
deriving bosonic statistics from the logarithmic axiom alone (Section 8.3). The
same logic excludes the existence of massless fermions — a retrodiction confirmed
by the discovery of neutrino mass (Section 8.4). The framework further predicts
the period, amplitude, and functional form of the Fermi-Dirac deviation from the
constraint, with all three quantitative predictions confirmed by the data of Shao
and Ma (Section 8.5). Additionally, quantum equations measured against the Benford
baseline should show characterizable deviation patterns that resolve at the
decoherence boundary; and gravitational equations, when brought to the constraint,
should satisfy it in a manner distinct from the other three forces — consistent with
gravity being the self-regulatory expression of the underlying logarithmic
constraint itself.

If confirmed, this framework would provide an empirical anchor for unification
physics — something string theory has never achieved — and would suggest that the
question of what existed before the Big Bang is answerable: the logarithmic
constraint that made the Big Bang possible.

The formula has been on the page since 1881. The question is whether we have been
reading it correctly.

---

## References

[1] S. Newcomb, "Note on the Frequency of Use of the Different Digits in Natural
Numbers," *American Journal of Mathematics*, vol. 4, no. 1, pp. 39-40, 1881.

[2] F. Benford, "The Law of Anomalous Numbers," *Proceedings of the American
Philosophical Society*, vol. 78, no. 4, pp. 551-572, 1938.

[3] J. Burke and E. Kincanon, "Benford's Law and Physical Constants: The
Distribution of Initial Digits," *American Journal of Physics*, vol. 59, no. 10,
pp. 952-954, 1991.

[4] D. Ni and Z. Ren, "Benford's Law and Half-Lives of Unstable Nuclei," *European
Physical Journal A*, vol. 38, pp. 251-255, 2008.

[5] L. Shao and B.-Q. Ma, "First Digit Distribution of Hadron Full Width," *Modern
Physics Letters A*, vol. 24, no. 30, pp. 2465-2474, 2009.

[6] L. Shao and B.-Q. Ma, "The Significant Digit Law in Statistical Physics,"
*Physica A*, vol. 389, no. 16, pp. 3109-3116, 2010.

[7] Y. Ralchenko and J.-C. Pain, "Benford's Law in Atomic Spectra and Opacity
Databases," *Journal of Quantitative Spectroscopy and Radiative Transfer*, vol. 322,
109010, 2024.

[8] T. Alexopoulos and S. Leontsinis, "Benford's Law in Astronomy," *Journal of
Astrophysics and Astronomy*, vol. 35, pp. 639-648, 2014.

[9] R. S. Pinkham, "On the Distribution of First Significant Digits," *Annals of
Mathematical Statistics*, vol. 32, no. 4, pp. 1223-1230, 1961.

[10] T. P. Hill, "Base-Invariance Implies Benford's Law," *Proceedings of the
American Mathematical Society*, vol. 123, no. 3, pp. 887-895, 1995.

[11] T. P. Hill, "A Statistical Derivation of the Significant-Digit Law,"
*Statistical Science*, vol. 10, no. 4, pp. 354-363, 1995.

[12] O. Kafri, "Entropy Principle in Direct Derivation of Benford's Law,"
arXiv:0901.3047, 2009.

[13] D. S. Lemons, "Thermodynamics of Benford's First Digit Law," *American Journal
of Physics*, vol. 87, no. 10, pp. 787-790, 2019.

[14] A. Burgos and A. Santos, "The Newcomb-Benford Law: Scale Invariance and a
Simple Markov Process Based on It," *American Journal of Physics*, vol. 89, no. 9,
pp. 851-861, 2021.

[15] A. Berger and T. P. Hill, "A Basic Theory of Benford's Law," *Probability
Surveys*, vol. 8, pp. 1-126, 2011.

[16] C. E. Shannon, "A Mathematical Theory of Communication," *Bell System Technical
Journal*, vol. 27, pp. 379-423 and 623-656, 1948.

[17] L. Boltzmann, "Uber die Beziehung zwischen dem zweiten Hauptsatze der
mechanischen Warmetheorie und der Wahrscheinlichkeitsrechnung," *Wiener Berichte*,
vol. 76, pp. 373-435, 1877.

[18] E. T. Jaynes, "Information Theory and Statistical Mechanics," *Physical Review*,
vol. 106, no. 4, pp. 620-630, 1957.

[19] K. G. Wilson, "Renormalization Group and Critical Phenomena. I.," *Physical
Review B*, vol. 4, pp. 3174-3183, 1971.

[20] E. Verlinde, "On the Origin of Gravity and the Laws of Newton," *Journal of
High Energy Physics*, 2011, 29, 2011.

[21] T. Jacobson, "Thermodynamics of Spacetime: The Einstein Equation of State,"
*Physical Review Letters*, vol. 75, pp. 1260-1263, 1995.

[22] M. Sambridge, H. Tkalcic, and A. Jackson, "Benford's Law in the Natural
Sciences," *Geophysical Research Letters*, vol. 37, L22301, 2010.

[23] A. Sen(De) and U. Sen, "Benford's Law Detects Quantum Phase Transitions
Similarly as Earthquakes," *Europhysics Letters*, vol. 95, 50008, 2011.

[24] A. D. Rane, U. Mishra, A. Biswas, A. Sen(De), and U. Sen, "Benford's Law
Gives Better Scaling Exponents in Phase Transitions of Quantum XY Models," *Physical
Review E*, vol. 90, 022144, 2014.

[25] R. Bousso, "The Holographic Principle," *Reviews of Modern Physics*, vol. 74,
pp. 825-874, 2002.

[26] S. W. Hawking, "Particle Creation by Black Holes," *Communications in
Mathematical Physics*, vol. 43, pp. 199-220, 1975.

[27] P. Calabrese and J. Cardy, "Entanglement Entropy and Quantum Field Theory,"
*Journal of Statistical Mechanics*, P06002, 2004.

[28] S. Ryu and T. Takayanagi, "Holographic Derivation of Entanglement Entropy from
AdS/CFT," *Physical Review Letters*, vol. 96, 181602, 2006.

[29] J. Maldacena, "The Large N Limit of Superconformal Field Theories and
Supergravity," *Advances in Theoretical and Mathematical Physics*, vol. 2,
pp. 231-252, 1998.

[30] W. K. T. Cho and B. J. Gaines, "Breaking the (Benford) Law: Statistical Fraud
Detection in Campaign Finance," *The American Statistician*, vol. 61, no. 3,
pp. 218-223, 2007.

[31] L. M. Leemis, B. W. Schmeiser, and D. L. Evans, "Survival Distributions
Satisfying Benford's Law," *The American Statistician*, vol. 54, no. 4,
pp. 236-241, 2000.

[32] S. N. Bernstein, "Sur les fonctions absolument monotones," *Acta Mathematica*,
vol. 52, pp. 1-66, 1929.

[33] M. Cong, M. Li, and B.-Q. Ma, "First Digit Law from Laplace Transform,"
*Physics Letters A*, vol. 383, 1836, 2019.

[34] Y. Fukuda et al. (Super-Kamiokande Collaboration), "Evidence for Oscillation
of Atmospheric Neutrinos," *Physical Review Letters*, vol. 81, pp. 1562-1567, 1998.

[35] Q. R. Ahmad et al. (SNO Collaboration), "Measurement of the Rate of
νe + d → p + p + e− Interactions Produced by 8B Solar Neutrinos at the Sudbury
Neutrino Observatory," *Physical Review Letters*, vol. 87, 071301, 2001.

[36] D. Lemons, N. Lemons, and W. Peter, "First Digit Oscillations," *Stats*,
vol. 4, pp. 595-601, 2021.

---

*Correspondence: chrisriner45@gmail.com*
